---
layout: post
title:  Feces
categories: [notes]
tags: [Metagenome]
---
	Fecal samples,
	What's in them?
	Hosts, microbes, their diets, environment contaminations? 
	Let's find out.
	- Mr. Peanutbatter

"""
What's in the total DNA in fecal samples? 

Here we use herbivore as an example, since it contains animal, plants and microbial DNA.

[Srivathsan 2016](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839110/#MOESM1) 

## Step 1: host identification

Srivathsan 2016 used an existing mitochondiral genome as reference and MITObim as a tool. Here they didn't have a problem for mis-idenfitication of feces so they know which mitochondiral genome to use. For me I could pretend all the samples we collected are from wildboar and see whether there is any difference in mapping coverage among samples.

1. What is this MITObim tool? 
mitochondrial baiting and iterative mapping, published on NAR in 2013 and cited by 776). Sounds promising!
2. Could I just use my usual bwa-samtools-coverage tool?

Decision: MITObim.

Installation tips:
1. It depends on MIRA. Install Mira4.0.2

Tried tutorial 1 and 2 and seems like we will just follow 2. 

Downloaded wild boar mitochondria genome [KX146493](https://www.ncbi.nlm.nih.gov/nuccore/KX146493.1), 16k. 

OK our own data. Needed to know more about [how to use mira](http://mira-assembler.sourceforge.net/docs/DefinitiveGuideToMIRA.html#chap_dataprep).

Some tips from mira author:

1. `for heavens' sake: do NOT try to clip or trim by quality yourself. Do NOT try to remove standard sequencing adaptors yourself. Just leave Illumina data alone! (really, I mean it)...Just make sure you use the [-CL:pec] (proposed_end_clip) option of MIRA.`
2. Reads needs to be interleaved! Are you kidding me? Such a pain. Using this [python script](https://github.com/ekg/interleave-fastq/blob/master/interleave-fastq).

	`for file in ../../0raw_data/PUUB*; do base=${file#../../0raw_data/}; echo $base; for data1 in $file/*_1.fq.gz; do data2=${data1%_1.fq.gz}_2.fastq.gz; interleave-fastq $data1 $data2 | gzip >> $base.interleaved.fq.gz; done; done` 
	
3. Run -quick option, which is what is described in tutorial II. 

	`MITObim.pl -start 1 -end 30 -sample PUUB01 -ref KX146493 -readpool PUUB01vtx.interleaved.fastq.gz -quick ../../KX146493.fasta`
	
	Note that the readpool here can only be .fastq.gz. Other suffixes such as .fq.gz won't be taken.
	
	This seems to take a long time. According to the author of MITObim, "For "well behaved" datasets the standard mapping assembly can be substituted by a de novo assembly (--denovo flag). Utilizing read pair information (--paired flag) can further speed up the reconstruction if run in de novo mode. This however will not decrease the number of necessary iterations in standard mapping mode.". So I run it both ways to see the difference for PUUB01.
	
	`nohup MITObim.pl -start 1 -end 30 -sample PUUB01 -ref KX146493 -readpool ../PUUB01vtx.interleaved.fastq.gz -quick ../../../../KX146493.fasta --denovo --paired > PUUB01_MITObim_denovo.log 2>&1 < /dev/null &`
	
	`for file in *.fastq.gz; do base=${file%.interleaved.fastq.gz}; cd $base; mv ../$file ./; MITObim.pl -start 1 -end 30 -sample $base -ref KX146493 -readpool $file -quick ../../../KX146493.fasta & cd ../; done`

The second one did not seem to run faster than the previous one. 

results are in the last iteration: e.g. PUUB01/iteration6/PUUB01-KX146493-it6_noIUPAC.fasta
Extract COX1 gene for ID:

	`python ~/build/script/extract_sequence.py -g /NAS_L1/huan/data/feed/wildboar/PUUB/1MITObim/interleaved/PUUB01/iteration6/PUUB01-KX146493-it6_noIUPAC.fasta -q ../../../KX146493_COX1.fasta`
	
Then blast the extracted sequence in NCBI, find the closest species. Results are documentedc in Wildboar.xlsx on sheet hostID.

If we didn't have a full mitochondria genome, could have followed tutorial III where only marker genes (such as COI) are required.

There are 4 samples that weren't able to extract COI sequence. Blast their longest contig in the final MITObim product instead. 

Their longest contigs all hit human mt genomes. This is misleading because this could be due to human contamination that is covering signals from wildboar. For example in PUUB07, when blasting another contig, it hits a wildboar sequence iwth 100% identity. Therefore I used that sequence as seed to carry out tutorial III, where you only need to provide a marker gene. This attempt failed with default setting (k=31) due to the fact that "your readpool does not contain any reads with reasonable match (k = 31) to your reference - Maybe you ll want to try different s
ettings or even a different reference?". k=31 is indeed pretty big. So started to step down:

k=29-25, did not work at all
k=23-9, one iteration

Used COX1 gene directly, same story. Stop trying this route but the host ID of those four samples remains unknown. Put it aside for now.


## Step 2: host genome, kneaddata

There are 11 assemblies availble for sus scrofa, however they all seems to be sus scrofa domestica since there are breed names attached. Chose the model one [Sscrofa11.1](ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/003/025/GCF_000003025.6_Sscrofa11.1).

Also need to remove human contamination.

There is a choice between bmtagger and bowtie2. Best Match Tagger (BMTagger) is an efficient tool that discriminates between human reads and microbial reads without doing an alignment of all reads to the human genome(https://www.westgrid.ca/support/software/bmtagger). Sounds very interesting. Maybe this could be used and done before the MITObim step. So updated pipeline:

1. bmtagger for removing human contamination.
2. MITObim reconstruction for those four poor samples.

But for now since we are on a time constrain to produce some stats on the microbial recovery, let's stick to bowtie.

1. Download human database: `kneaddata_database --download human_genome bowtie2 human`
2. Make costumed wildboar database: `bowtie2-build GCF_000003025.6_Sscrofa11.1_genomic.fna wildboar`

	`for dir in ../../1concatenated/*; do echo $dir; base=${dir#../../1concatenated/}; kneaddata --input $dir/${base}_raw_1.fq.gz --input $dir/${base}_raw_2.fq.gz -db /NAS_L1/ref/bowtie/human -db /NAS_L1/ref/bowtie/wildboar --output ${base}_kneaddata & done`
	
	kneaddata needs to first decompress the data files. This is very time and disk-space consuming. Then it also has to reformat the whole thing for pairend purposes, which will take even longer than decompressing! Then it spends one hour to count the number of reads. Enough!
	
	It was running last night but due to low disk space did not finish.
	
	Turns out not much human or wildboar data. Some samples do have more than others.
	 
## Step 3: composite genome recovery 

Step 1. megahit

	for dir in ../2kneaddata/wildboar_human/*; do base=${dir#../2kneaddata/wildboar_human/}; ba=${base%_kneaddata}; echo $ba; nohup megahit -1 $dir/${ba}_kneaddata.raw_1_kneaddata_paired_1.fastq -2 $dir/${ba}_kneaddata.raw_1_kneaddata_paired_2.fastq -r $dir/${ba}_kneaddata.raw_1_kneaddata_unmatched_1.fastq -r $dir/${ba}_kneaddata.raw_1_kneaddata_unmatched_2.fastq -o $ba > ${ba}_megahit.log 2>&1 < /dev/null & done

Step 2. MaxBin

Install maxbin based on its readme came with the package.

	for file in ../3megahit/*/; do echo $file; base=${file#../3megahit/}; ba=${base%/}; echo $ba; nohup ~/build/MaxBin-2.2.7/run_MaxBin.pl -contig ${file}final.contigs.fa -reads ../2kneaddata/wildboar_human/${ba}_kneaddata/${ba}_kneaddata.raw_1_kneaddata_paired_1.fastq -reads2 ../2kneaddata/wildboar_human/${ba}_kneaddata/${ba}_kneaddata.raw_1_kneaddata_paired_2.fastq -reads3 ../2kneaddata/wildboar_human/${ba}_kneaddata.raw_1_kneaddata_unmatched_1.fastq -reads4 ../2kneaddata/wildboar_human/${ba}_kneaddata.raw_1_kneaddata_unmatched_2.fastq -out ${ba}_maxbin -thread 30 > ${ba}_MaxBin.log 2>&1 < /dev/null & done
## Step 4: kraken2 microbial community ID

Before this many use sortmeRNA to separate the RNA reads first.

	for file in ../0raw_data/*; do base=${file#../0raw_data/}; echo $base; nohup kraken2 --db /NAS_L1/ref/kraken2/kranken2_standard --threads 32 --output $base.kraken2_names.txt --use-names --report $base.kraken2.report.txt --paired ../2kneaddata/wildboar_human/${base}_kneaddata/${base}_kneaddata.raw_1_kneaddata_paired_1.fastq ../2kneaddata/wildboar_human/${base}_kneaddata/${base}_kneaddata.raw_1_kneaddata_paired_2.fastq > ${base}_kraken2.log 2>&1 < /dev/null & done
	
Ushida's datase:

	for file in *; do nohup kraken2 --db /NAS_L1/ref/kraken2/kranken2_standard --threads 32 --output $file.kraken2_names.txt --use-names --report $file.kraken2.report.txt --bzip2-compressed --paired $file/*_1.fastq.bz2 $file/*_2.fastq.bz2 > ${file}_kraken2.log 2>&1 < /dev/null & done
	
Plotting use genus-level info only and exclude low percentage ones (>=0.01).

	for file in *.txt; do base=${file%.report.txt}; grep '\tG\t' $file | grep -v '0.00' > $base.genus.txt; done
	


## Step 4: diet id: unclassified reads id against a plant marker gene database

10-20G for Amrita's study.

## How much data is good enough?



